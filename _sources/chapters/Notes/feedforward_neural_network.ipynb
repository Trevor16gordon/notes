{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks\n",
    "\n",
    "![Example](images/feed_forward_example.png)\n",
    "\n",
    "# Important Papers\n",
    "\\begin{align}\n",
    "\\dfrac{2L}{\\partial \\widehat{y}}RSS=2\\times \\left( y-\\widehat{y}\\right) \n",
    "\\begin{aligned}\\dfrac{2L}{\\partial \\widehat{y}}RSS=2\\times \\left( y-\\widehat{y}\\right) \\\\\n",
    "\\dfrac{2\\widehat{y}}{2h^{2}}signoid=\\sigma \\left( \\widehat{y}\\right) \\left( 1-\\sigma \\left( \\widehat{y}\\right) \\right) \\\\\n",
    "\\dfrac{\\partial h2}{\\partial W2}=21\\\\\n",
    "\\dfrac{2h^{2}}{\\partial z^{1}}=W2\\\\\n",
    "\\dfrac{221}{2h1}=f|^{1}=f|_{s:gmoid}^{1}=\\sigma \\left( z\\right) )\\left( 1-\\sigma \\left( zI\\right) \\right) \\end{aligned}\n",
    "\\end{align}\n",
    "## Derive dL/dW2 and dL/dc2\n",
    "We want to update W1, W2, c1, c2.\n",
    "To do this we need to update using dL/dW1, dL/dW2, dL/dc1, dL/dc1\n",
    "These are calculated using the chain rule\n",
    "\\begin{align}\n",
    "  \\\\{\\frac{\\partial L}{\\partial yhat}} RSS \n",
    "    &= 2*(y-yhat)\n",
    "  \\\\{\\frac{\\partial yhat}{\\partial h2}} sigmoid \n",
    "    &= \\sigma (yhat) (1 - \\sigma (yhat))\n",
    "  \\\\{\\frac{\\partial h2}{\\partial W2}} \n",
    "    &= z1\n",
    "  \\\\{\\frac{\\partial h2}{\\partial C2}} \n",
    "    &= 1\n",
    "  \\\\{\\frac{\\partial h2}{\\partial z1}} \n",
    "    &= W2\n",
    "  \\\\{\\frac{\\partial z1}{\\partial h1}} \n",
    "    &= f1' =f1'sigmoid = \\sigma (z1) (1 - \\sigma (z1))\n",
    "  \\\\{\\frac{\\partial h1}{\\partial W1}} \n",
    "    &= x1\n",
    "  \\\\{\\frac{\\partial h1}{\\partial C1}} \n",
    "    &= 1\n",
    "\\end{align}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example Neural Network From Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"Trevor Gordon. 2021\n",
    "    Implementation of a feed forward neural network from scratch.\n",
    "    Drawing largely from https://dafriedman97.github.io/mlbook/content/c7/concept.html\n",
    "    and https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "    \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class FeedForwardNeuralNetwork():\n",
    "    \"\"\"Feed Forward Neural Network\n",
    "    This FFNN is fixed to have one hidden layer. It is defined by weights self.W1 and self.W2 and\n",
    "    biases self.c1 and self.c2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Init function\n",
    "        Class variables:\n",
    "            self.n_hidden (int): Number of neurons in single hidden layer\n",
    "            self.f1 (func): Activation function to apply to hidden layer output before next layer\n",
    "                Only supporting sigmoid right now\n",
    "        \"\"\"\n",
    "        self.n_hidden = 1\n",
    "        self.f1 = self.sigmoid\n",
    "        self.f1_prime = self.sigmoid_prime\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid function to bound output between 0 and 1\"\"\"\n",
    "        return 1 / (1 + np.exp(-1*x))\n",
    "\n",
    "    def sigmoid_prime(self, x):\n",
    "        \"\"\"Derivative of the sigmoid function simplifies to this.\"\"\"\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def initialize_params(self, len_input, len_hidden, len_output):\n",
    "        \"\"\"Initialize weights and biases randomly.\n",
    "        Args:\n",
    "            len_input (int): Number of neurons in X input layer\n",
    "            len_hidden (int): Number of neurons in hidden layer\n",
    "            len_output (int): Number of neurons in y output layer\n",
    "        Returns:\n",
    "            weights_1 (np.array): Rows for neurons in hidden layer and columns for \n",
    "                neurons in X input layer.\n",
    "            weights_2 (np.array): Rows for neurons in y output layer and columns for neurons in\n",
    "                hidden layer\n",
    "            biases_1 (np.array): Rows for neurons in hidden layer and 1 column\n",
    "            biases_2 (np.array): Rows for neurons in output layer and 1 column\n",
    "        \"\"\"\n",
    "        weights_1 = np.random.randn(len_hidden, len_input)/5\n",
    "        biases_1 = np.random.randn(len_hidden, 1)/5\n",
    "        weights_2 = np.random.randn(len_output, len_hidden)/5\n",
    "        biases_2 = np.random.randn(len_output, 1)/5\n",
    "        return weights_1, biases_1, weights_2, biases_2\n",
    "\n",
    "    def update_network_states(self):\n",
    "        \"\"\"Using weights, biases and activation function, calculate hidden layer and output\n",
    "        Returns:\n",
    "            h1 (np.array): Hidden layer with shape = (len_hidden, num_observations)\n",
    "            z1 (np.array): Activated hidden layer with shape = (len_hidden, num_observations)\n",
    "            h2 (np.array): Hidden layer 2 with shape = (len_output, num_observations)\n",
    "            yhat (np.array): Output layer with shape = (len_output, num_observations)\n",
    "        \"\"\"\n",
    "        h1 = np.dot(self.W1, self.X.T) + self.c1\n",
    "        z1 = self.f1(h1)\n",
    "        h2 = np.dot(self.W2, z1) + self.c2\n",
    "        yhat = self.f1(h2)\n",
    "        return h1, z1, h2, yhat\n",
    "\n",
    "    def get_loss_gradient_one_observations(self,\n",
    "                                           y_one_obsv,\n",
    "                                           yhat_one_obsv,\n",
    "                                           h2_one_obsv,\n",
    "                                           h1_one_obsv,\n",
    "                                           z1_one_obsv,\n",
    "                                           x_input_one_obsv,\n",
    "                                           len_hidden):\n",
    "        \"\"\"Given a single observation find the loss gradient.\n",
    "        Derivations for all of these are shown in the images folder.\n",
    "        Args:\n",
    "            y_one_obsv (np.array): Single output. Shape = (len_output, 1)\n",
    "            yhat_one_obsv (np.array): Single predicted output given current weights/biases\n",
    "            h2_one_obsv (np.array): Current h2 hidden layer for single input x and weights/biases\n",
    "            h1_one_obsv (np.array): Current h1 hidden layer for single input x and weights/biases\n",
    "            z1_one_obsv (np.array): Current z1 value for single input x and weights/biases\n",
    "            x_input_one_obsv (np.array): Single x input\n",
    "            len_hidden (int): Number of neurons in hidden layer\n",
    "        Returns:\n",
    "            dL_dW1_one_obvs (np.array): Partial grad showing direction to move. Same shape as W1 \n",
    "            dL_dc1_one_obvs (np.array): Partial grad showing direction to move. Same shape as c1 \n",
    "            dL_dW2_one_obvs (np.array): Partial grad showing direction to move. Same shape as W2 \n",
    "            dL_dc2_one_obvs (np.array): Partial grad showing direction to move. Same shape as c2\n",
    "        \"\"\"\n",
    "\n",
    "        len_input = len(x_input_one_obsv)\n",
    "        len_output = len(y_one_obsv)\n",
    "\n",
    "        # Start at output by calculation error\n",
    "        dL_dyhat = -2*(y_one_obsv - yhat_one_obsv).T  # (1, len_output)\n",
    "\n",
    "        ## LAYER 2 ##\n",
    "        # dyhat_dh2.shape = (len_output, len_output)\n",
    "        dyhat_dh2 = np.diag(self.f1_prime(h2_one_obsv))\n",
    "\n",
    "        # dh2_dc2.shape =  (len_output, len_output)\n",
    "        # dh2_dc2 = 1 because bias is simply added\n",
    "        dh2_dc2 = np.eye(len_output)\n",
    "\n",
    "        # dh2_dW2.shape = (len_output, (len_output, len_hidden))\n",
    "        dh2_dW2 = np.zeros((len_output, len_output, len_hidden))\n",
    "        for i in range(len_output):\n",
    "            dh2_dW2[i] = z1_one_obsv\n",
    "\n",
    "        # dh2_dz1.shape = (len_output, len_hidden)\n",
    "        dh2_dz1 = self.W2\n",
    "\n",
    "        ## LAYER 1 ##\n",
    "        # dz1_dh1.shape = (len_hidden, len_hidden)\n",
    "        dz1_dh1 = np.diag(self.f1(h1_one_obsv)*(1-self.f1(h1_one_obsv)))\n",
    "\n",
    "        # dh1_dc1.shape = (len_hidden, len_hidden)\n",
    "        dh1_dc1 = np.eye(len_hidden)\n",
    "\n",
    "        # dh1_dW1\n",
    "        # (len_hidden, (len_hidden, D_X))\n",
    "        dh1_dW1 = np.zeros((len_hidden, len_hidden, len_input))\n",
    "        for i in range(len_hidden):\n",
    "            dh1_dW1[i] = x_input_one_obsv\n",
    "\n",
    "        ## DERIVATIVES W.R.T. LOSS ##\n",
    "        dL_dh2 = dL_dyhat @ dyhat_dh2\n",
    "        dL_dW2_one_obvs = dL_dh2 @ dh2_dW2\n",
    "        dL_dc2_one_obvs = dL_dh2 @ dh2_dc2\n",
    "\n",
    "        dL_dh1 = dL_dh2 @ dh2_dz1 @ dz1_dh1\n",
    "\n",
    "        dL_dW1_one_obvs = dL_dh1 @ dh1_dW1\n",
    "        dL_dc1_one_obvs = dL_dh1 @ dh1_dc1\n",
    "        # dL_dc1_one_obvs = dL_dc1_one_obvs.reshape(len(dL_dc1_one_obvs), -1)\n",
    "        # dL_dc2_one_obvs = dL_dc2_one_obvs.reshape(len(dL_dc2_one_obvs), -1)\n",
    "        return dL_dW1_one_obvs, dL_dc1_one_obvs, dL_dW2_one_obvs, dL_dc2_one_obvs\n",
    "\n",
    "    def get_loss_gradient_iter_over_observations(self, len_hidden):\n",
    "        \"\"\"Get the cumulative gradient while iterating over each observation at a time\n",
    "        Args:\n",
    "            len_hidden (int): Number of neurons in hidden layer\n",
    "        Returns:\n",
    "            dL_dW1 (np.array): Partial grad showing direction to move. Same shape as W1 \n",
    "            dL_dc1 (np.array): Partial grad showing direction to move. Same shape as c1 \n",
    "            dL_dW2 (np.array): Partial grad showing direction to move. Same shape as W2 \n",
    "            dL_dc2 (np.array): Partial grad showing direction to move. Same shape as c2\n",
    "        \"\"\"\n",
    "        dL_dW2 = 0\n",
    "        dL_dc2 = 0\n",
    "        dL_dW1 = 0\n",
    "        dL_dc1 = 0\n",
    "        num_observations = len(self.X)\n",
    "        for n in range(num_observations):\n",
    "            # Slice y and yhat by observation number\n",
    "            # dL_dyhat\n",
    "            y_one_obsv = self.y[n]  # (1, len_output)\n",
    "            yhat_one_obsv = self.yhat[:, n]  # (1, len_output)\n",
    "            h2_one_obsv = self.h2[:, n]\n",
    "            h1_one_obsv = self.h1[:, n]\n",
    "            z1_one_obsv = self.z1[:, n]\n",
    "            x_input_one_obsv = self.X[n]\n",
    "            (dL_dW1_one_obvs, dL_dc1_one_obvs, dL_dW2_one_obvs,\n",
    "             dL_dc2_one_obvs) = self.get_loss_gradient_one_observations(\n",
    "                y_one_obsv, yhat_one_obsv, h2_one_obsv, h1_one_obsv,\n",
    "                z1_one_obsv, x_input_one_obsv, len_hidden)\n",
    "            dL_dW2 += dL_dW2_one_obvs\n",
    "            dL_dc2 += dL_dc2_one_obvs\n",
    "            dL_dW1 += dL_dW1_one_obvs\n",
    "            dL_dc1 += dL_dc1_one_obvs\n",
    "        return dL_dW1, dL_dc1, dL_dW2, dL_dc2\n",
    "\n",
    "    def get_loss_gradient(self, len_hidden):\n",
    "        \"\"\"Get the loss gradient.\n",
    "        This defaults to get_loss_gradient_iter_over_observations right now. In a future version\n",
    "        this should be implemented while iterating on mini batches to improve speed.\n",
    "        Args:\n",
    "            len_hidden (int): Number of neurons in hidden layer\n",
    "        Returns:\n",
    "            dL_dW1 (np.array): Partial grad showing direction to move. Same shape as W1 \n",
    "            dL_dc1 (np.array): Partial grad showing direction to move. Same shape as c1 \n",
    "            dL_dW2 (np.array): Partial grad showing direction to move. Same shape as W2 \n",
    "            dL_dc2 (np.array): Partial grad showing direction to move. Same shape as c2\n",
    "        \"\"\"\n",
    "        (dL_dW1,\n",
    "        dL_dc1,\n",
    "        dL_dW2,\n",
    "        dL_dc2) = self.get_loss_gradient_iter_over_observations(len_hidden)\n",
    "        return dL_dW1, dL_dc1, dL_dW2, dL_dc2\n",
    "\n",
    "    def fit(self, X, y, len_hidden, grad_step=1e-5, n_iter=1000, seed=None):\n",
    "        \"\"\"Fit a feedforward neural network to the test set given.\n",
    "        Args:\n",
    "            X (np.array): Input data with columns for elements of a single input\n",
    "            and rows for number of observations. X.shape = (num_observations, data_per_observation)\n",
    "            y (np.array): Correct prediction for the input samples. y.shape = (num_observations, len_output)\n",
    "            len_hidden (int): Number of neurons in input layer.\n",
    "            grad_step (float, optional): Step for updating weights with gradient vectors. Defaults to 1e-5.\n",
    "            n_iter (int, optional): Number of iterations. Defaults to 1e3.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        # Reshaping with -1 make sure theres a second dim of at least 1\n",
    "        self.y = y.reshape(len(y), -1)\n",
    "        len_input = X.shape[1]\n",
    "        len_output = self.y.shape[1]\n",
    "        self.W1, self.c1, self.W2, self.c2 = self.initialize_params(\n",
    "            len_input, len_hidden, len_output)\n",
    "        self.h1, self.z1, self.h2, self.yhat = self.update_network_states()\n",
    "        print(f\"Starting to fit for {n_iter} iterations\")\n",
    "        for i in tqdm.tqdm(range(n_iter)):\n",
    "            # Adjust weights and biases in the direction of the negative gradient of the loss function\n",
    "\n",
    "            dL_dW1, dL_dc1, dL_dW2, dL_dc2 = self.get_loss_gradient(len_hidden)\n",
    "\n",
    "            self.W1 -= grad_step * dL_dW1\n",
    "            self.W2 -= grad_step * dL_dW2\n",
    "            self.c1 -= grad_step * dL_dc1.reshape(-1, 1)\n",
    "            self.c2 -= grad_step * dL_dc2.reshape(-1, 1)\n",
    "\n",
    "            self.h1, self.z1, self.h2, self.yhat = self.update_network_states()\n",
    "        return True\n",
    "\n",
    "    def predict(self, X_predict):\n",
    "        \"\"\"Predict output for given input after model has been fit.\n",
    "        Args:\n",
    "            X_predict (np.array): Input data with columns for elements of a single input. \n",
    "            and  rows for each prediction. X.shape = (num_to_predict, data_per_observation)\n",
    "        Returns:\n",
    "            np.array: Predictions. y.shape = (data_per_output, num_to_predict)\n",
    "        \"\"\"\n",
    "        self.X = X_predict\n",
    "        h1, z1, h2, yhat = self.update_network_states()\n",
    "        return yhat"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('sandbox': virtualenv)"
  },
  "interpreter": {
   "hash": "2f689eec9011d9f30cc59914a408e328b5fb277dd139badbf6f83f80eeab63ed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}